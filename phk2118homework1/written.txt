1. Weiss, 2.1

From slowest to fastest growth rates:
	2/N, 37, sqrt(N), NloglogN, N, NlogN, Nlog(N^2), N(logN)^2, N^1.5, N^2, (N^2)logN, N^3, 2^(N/2), 2^N
	None of these functions grow at the same rate.



2. Weiss, 2.6

a.

On day 1: fine is $2.
   day 2: $4.
   day 3: $16.
	...

   day N: $2^(2^(N-1)).



b.

D = 2^(2^(N-1))			solve for N in terms of D.
logD = 2^(N-1)
log(logD) = N-1
N = log(logD) + 1
--> the answer is O(loglogD).







3. 

a.

line 1 has 2 operations: declaration and assignment.

line 4 has 2 operations: addition and assignment.
according to rule of nested loops:
	max running time for loop = 2(N*23)
	    running time for line 1 = 2.

	total running time: 2(N*23) + 2
	--> O(N).



b.

line 1 has 2 operations: declaration and assignment.

line 4 has 2 operations: addition and assignment.
according to rule of nested loops:
	max running time for loop = 2(N(N-i))
	    running time for line 1 = 2.

	total running time: 2*(N(N-i)) + 2
		**note: N-i could be small, but big-Oh is a worst case approximation. Therefore, approximate N-i to be N.
	--> O(N^2).



c.	

Let T(N) be the running time of this algorithm.
If N<=k, then the running time of this algorithm is a constant value: 1. (due to the test in the if statement).
	since Big-Oh is an upper estimate, this is irrelevant.

If N>K, then T(N) = T(N/k) + 2, from previous if-statement and addition in this if-statement. ignore cost of method calls and returns.
repeat this relation until N/k/k/... < k.

as N grows, T(N) grows only due to the ratio between N and k. for a constant k, a linear growth in N will result in a linear growth in running time.
--> O(N).



alternate method: 

public int foo(int n, int k) {				
	if(n<=k)					
		return 1;				
	else						
		return foo(n/k, k) + 1;			
}


can be rewritten as:

public int foo(int n, int k) {
	if(n<=k)
		n = 1;
	while(n>k)
		n = n/k + 1;

	return n;	
}

	Analysis of this while loop: the running time is at most the running times of the statements * the largest possible number of iterations.
		the running time of the statements is 2: assignment and division.
		one upper bound for the largest possible number of iterations is N/k.
		this results in a big-Oh of O(N).






4. Weiss, 2.11

0.5 ms for input = 100.
??? ms for input = 500 for:

a. linear

5x increase in input corresponds to 5x increase in running time.
(0.5 ms)(5) = 2.5ms



b. O(N log N)

NlogN vs (5N)log(5N)
	 = (5N)(log5 + logN)
	 = (5N)(logN)		because log5 term is negligible

--> 5x increase in input corresponds to 5x increase in running time.

(0.5 ms)(5) = 2.5ms



c. quadratic

5x increase in input corresponds to 25x increase in running time.
(0.5 ms)(25) = 12.5ms



d. cubic

5x increase in input corresponds to 125x increase in running time.
(0.5 ms)(125) = 62.5ms









5. Weiss, 2.15

isEqual = 0;

for (i = 1; i <= N, i++)
	if (A_[i] == i)
		isEqual = 1;

the running time of this algorithm is 2N from rule of for loops.
--> O(N).